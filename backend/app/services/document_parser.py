"""
æ–‡æ¡£è§£ææœåŠ¡

åŠŸèƒ½ï¼š
1. æ”¯æŒ docx, md, pdf æ ¼å¼æ–‡æ¡£è§£æ
2. æ™ºèƒ½å¤šå±‚æ¬¡åˆ‡ç‰‡ç­–ç•¥
3. å›¾ç‰‡æå–å’ŒåŸä½ç½®æ’å…¥
"""

import hashlib
import os
import re
import zipfile
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any, Dict, List

# ==================== æ•°æ®ç»“æ„ ====================


@dataclass
class DocumentChunk:
    """æ–‡æ¡£å—æ•°æ®ç»“æ„"""

    chunk_id: str  # å—å”¯ä¸€ID
    text: str  # æ–‡æœ¬å†…å®¹
    source_file: str  # æ¥æºæ–‡ä»¶
    section: str  # æ‰€å±ç« èŠ‚
    page_number: int  # é¡µç ï¼ˆå¦‚æœ‰ï¼‰
    images: List[str]  # å…³è”å›¾ç‰‡è·¯å¾„åˆ—è¡¨
    metadata: Dict[str, Any]  # å…¶ä»–å…ƒæ•°æ®

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)


# ==================== é…ç½®ç±» ====================


@dataclass
class ChunkingConfig:
    """åˆ‡ç‰‡é…ç½®å‚æ•°"""

    # åŸºç¡€åˆ‡ç‰‡å‚æ•°
    max_chunk_size: int = 800  # æœ€å¤§å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
    min_chunk_size: int = 100  # æœ€å°å—å¤§å°ï¼ˆé¿å…è¿‡å°çš„å—ï¼‰
    chunk_overlap: int = 100  # å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°

    # åˆ‡ç‰‡ç­–ç•¥
    split_by_title: bool = True  # æ˜¯å¦æŒ‰æ ‡é¢˜åˆ‡åˆ†
    split_by_paragraph: bool = True  # æ˜¯å¦æŒ‰æ®µè½åˆ‡åˆ†
    force_max_size: bool = True  # æ˜¯å¦å¼ºåˆ¶é™åˆ¶æœ€å¤§é•¿åº¦

    # æ ‡é¢˜è¯†åˆ«æ¨¡å¼
    title_patterns: List[str] = None  # é¢å¤–çš„æ ‡é¢˜è¯†åˆ«æ­£åˆ™æ¨¡å¼

    # å›¾ç‰‡å¤„ç†
    distribute_images_evenly: bool = True  # æ˜¯å¦å‡åŒ€åˆ†é…å›¾ç‰‡åˆ°å„chunk

    def __post_init__(self):
        """åˆå§‹åŒ–é»˜è®¤å€¼"""
        if self.title_patterns is None:
            # ä¸­æ–‡æ ‡é¢˜æ¨¡å¼ï¼šä¸€ã€äºŒã€1. 2. ç¬¬ä¸€ç«  ç­‰
            self.title_patterns = [
                r"^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€\.]\s*.+",  # ä¸€ã€ æˆ– ä¸€.
                r"^\d+[ã€\.]\s*.+",  # 1ã€ æˆ– 1.
                r"^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚éƒ¨åˆ†]\s*.+",  # ç¬¬ä¸€ç« 
                r"^[ï¼ˆ\(]\d+[ï¼‰\)]\s*.+",  # (1) æˆ– ï¼ˆ1ï¼‰
            ]


# ==================== ä¼˜åŒ–çš„æ–‡æ¡£è§£æå™¨ ====================


class OptimizedDocumentParser:
    """
    ä¼˜åŒ–çš„æ–‡æ¡£è§£æå™¨ - æ”¯æŒæ™ºèƒ½åˆ‡ç‰‡

    ä¸»è¦ä¼˜åŒ–ï¼š
    1. å¤šå±‚æ¬¡åˆ‡ç‰‡ï¼šæ ‡é¢˜ > æ®µè½ > å›ºå®šé•¿åº¦
    2. é•¿åº¦æ§åˆ¶ï¼šä¿è¯ chunk åœ¨åˆç†èŒƒå›´
    3. é‡å æœºåˆ¶ï¼šæé«˜æ£€ç´¢å¬å›ç‡
    4. å¢å¼ºæ ‡é¢˜è¯†åˆ«ï¼šæ”¯æŒä¸­æ–‡æ ‡é¢˜æ¨¡å¼
    """

    def __init__(
        self,
        image_output_dir: str = "./extracted_images",
        config: ChunkingConfig = None,
    ):
        """
        åˆå§‹åŒ–è§£æå™¨

        Args:
            image_output_dir: å›¾ç‰‡æå–è¾“å‡ºç›®å½•
            config: åˆ‡ç‰‡é…ç½®
        """
        self.image_output_dir = Path(image_output_dir)
        self.image_output_dir.mkdir(parents=True, exist_ok=True)
        self.config = config or ChunkingConfig()

    def parse(self, file_path: str) -> List[DocumentChunk]:
        """
        è§£ææ–‡æ¡£ï¼Œè¿”å›å¸¦å›¾ç‰‡å…³è”çš„æ–‡æœ¬å—

        Args:
            file_path: æ–‡æ¡£è·¯å¾„

        Returns:
            DocumentChunk åˆ—è¡¨
        """
        file_path = Path(file_path)
        suffix = file_path.suffix.lower()

        if suffix == ".docx":
            return self._parse_docx(file_path)
        elif suffix == ".md":
            return self._parse_markdown(file_path)
        elif suffix == ".pdf":
            return self._parse_pdf(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}")

    def parse_directory(
        self, dir_path: str, extensions: List[str] = None
    ) -> List[DocumentChunk]:
        """
        è§£æç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£

        Args:
            dir_path: ç›®å½•è·¯å¾„
            extensions: è¦å¤„ç†çš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ï¼Œé»˜è®¤ ['.docx', '.md']

        Returns:
            æ‰€æœ‰æ–‡æ¡£çš„ DocumentChunk åˆ—è¡¨
        """
        if extensions is None:
            extensions = [".docx", ".md"]

        dir_path = Path(dir_path)
        all_chunks = []

        for ext in extensions:
            for file_path in dir_path.glob(f"*{ext}"):
                try:
                    chunks = self.parse(str(file_path))
                    all_chunks.extend(chunks)
                    print(f"âœ… è§£æå®Œæˆ: {file_path.name} ({len(chunks)} ä¸ªå—)")
                except Exception as e:
                    print(f"âŒ è§£æå¤±è´¥: {file_path.name} - {e}")

        return all_chunks

    # ==================== Word æ–‡æ¡£è§£æ ====================

    def _parse_docx(self, file_path: Path) -> List[DocumentChunk]:
        """
        è§£æ Word æ–‡æ¡£

        ç®€åŒ–é€»è¾‘ï¼šç›´æ¥æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼Œåªå¯¹å›¾ç‰‡åšå®šä½
        """
        try:
            from docx import Document
            from docx.oxml.ns import qn
        except ImportError:
            raise ImportError("è¯·å®‰è£… python-docx: pip install python-docx")

        doc = Document(str(file_path))
        doc_name = file_path.stem

        # æ„å»ºå…ƒç´ åˆ—è¡¨ï¼ŒåŒ…å«æ–‡æœ¬å’Œå›¾ç‰‡çš„ä½ç½®ä¿¡æ¯
        elements = []
        image_index = 0

        # éå†æ®µè½ï¼Œæå–æ–‡æœ¬å’Œå›¾ç‰‡
        for para in doc.paragraphs:
            # æ£€æŸ¥æ®µè½æ ·å¼ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºæ ‡é¢˜
            style_name = para.style.name if para.style else ""
            is_heading = style_name.startswith("Heading") or style_name.startswith(
                "æ ‡é¢˜"
            )

            # æ£€æŸ¥æ®µè½ä¸­çš„å›¾ç‰‡
            para_images = []
            for run in para.runs:
                drawing_elements = run._element.findall(
                    ".//a:blip",
                    {"a": "http://schemas.openxmlformats.org/drawingml/2006/main"},
                )
                for drawing in drawing_elements:
                    embed_id = drawing.get(qn("r:embed"))
                    if embed_id:
                        try:
                            image_part = doc.part.related_parts[embed_id]
                            image_ext = image_part.content_type.split("/")[-1]
                            if image_ext == "jpeg":
                                image_ext = "jpg"
                            image_index += 1
                            image_name = f"{doc_name}_image{image_index}.{image_ext}"
                            output_path = self.image_output_dir / image_name

                            with open(output_path, "wb") as f:
                                f.write(image_part.blob)

                            para_images.append(str(output_path))
                        except Exception as e:
                            print(f"  âš ï¸  æå–å›¾ç‰‡å¤±è´¥: {e}")

            # æ·»åŠ å…ƒç´ ï¼ˆç›´æ¥æå–æ‰€æœ‰æ–‡æœ¬ï¼‰
            text = para.text.strip()
            if text or para_images:
                elements.append(
                    {
                        "type": "heading" if is_heading else "paragraph",
                        "text": text,
                        "images": para_images,
                    }
                )

        # å¤„ç†è¡¨æ ¼ä¸­çš„å†…å®¹
        table_text_count = 0
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    cell_text = cell.text.strip()
                    if cell_text:
                        table_text_count += 1
                        elements.append(
                            {
                                "type": "paragraph",
                                "text": cell_text,
                                "images": [],
                            }
                        )

        print(
            f"  ğŸ“Š python-docx è§£æ: {len(elements)} ä¸ªå…ƒç´ ï¼Œ{image_index} å¼ å›¾ç‰‡ï¼Œ{table_text_count} ä¸ªè¡¨æ ¼æ–‡æœ¬"
        )

        # ä½¿ç”¨ä¼˜åŒ–çš„åˆ‡ç‰‡ç­–ç•¥
        return self._docx_elements_to_chunks(
            elements=elements,
            source_file=str(file_path),
        )

    def _docx_elements_to_chunks(
        self,
        elements: List[Dict],
        source_file: str,
    ) -> List[DocumentChunk]:
        """
        å°† python-docx è§£æçš„å…ƒç´ è½¬æ¢ä¸º chunks

        ç®€åŒ–é€»è¾‘ï¼šç›´æ¥æŒ‰æ ‡é¢˜åˆ†ç»„ï¼Œå›¾ç‰‡åœ¨åŸä½ç½®æ’å…¥å ä½ç¬¦
        """
        chunks = []
        current_section = "æ–‡æ¡£å¼€å§‹"

        # æŒ‰æ ‡é¢˜åˆ†ç»„
        sections = []
        section_data = {
            "section": current_section,
            "texts": [],
            "images": [],
            "page": 0,
        }

        for elem in elements:
            elem_type = elem["type"]
            text = elem["text"]
            images = elem.get("images", [])

            # æ£€æŸ¥æ˜¯å¦ä¸ºæ ‡é¢˜
            is_title = elem_type == "heading"

            # é¢å¤–æ£€æŸ¥ï¼šä¸­æ–‡æ ‡é¢˜æ¨¡å¼
            if (
                not is_title
                and text
                and self.config.split_by_title
                and self._is_custom_title(text)
            ):
                is_title = True

            if is_title and text:
                # ä¿å­˜ä¹‹å‰çš„ section
                if section_data["texts"] or section_data["images"]:
                    sections.append(section_data)

                # æ–° section
                current_section = text
                section_data = {
                    "section": current_section,
                    "texts": [],
                    "images": [],
                    "page": 0,
                }
            else:
                # æ·»åŠ æ–‡æœ¬åˆ°å½“å‰ section
                if text:
                    section_data["texts"].append(text)
                # åœ¨æ–‡æœ¬ä¸­æ’å…¥å›¾ç‰‡å ä½ç¬¦
                if images:
                    for img_path in images:
                        placeholder = f"[IMG:{img_path}]"
                        section_data["texts"].append(placeholder)
                        section_data["images"].append(img_path)
                        print(f"  ğŸ–¼ï¸  å›¾ç‰‡å·²å®šä½: {os.path.basename(img_path)}")

        # ä¿å­˜æœ€åä¸€ä¸ª section
        if section_data["texts"] or section_data["images"]:
            sections.append(section_data)

        # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°ä»»ä½• sectionï¼Œåˆ›å»ºé»˜è®¤ section
        if not sections:
            all_texts = [e["text"] for e in elements if e["text"]]
            all_images = []
            for e in elements:
                all_images.extend(e.get("images", []))
            sections = [
                {
                    "section": "æ–‡æ¡£å†…å®¹",
                    "texts": all_texts,
                    "images": all_images,
                    "page": 0,
                }
            ]

        total_images = sum(len(s["images"]) for s in sections)
        print(f"  ğŸ“Š è¯†åˆ«åˆ° {len(sections)} ä¸ªç« èŠ‚ï¼Œ{total_images} å¼ å›¾ç‰‡å·²å®šä½")

        # å¯¹æ¯ä¸ª section åº”ç”¨é•¿åº¦é™åˆ¶å’Œé‡å 
        for section_info in sections:
            section_text = "\n".join(section_info["texts"])
            section_images = section_info.get("images", [])

            # å¦‚æœ section è¿‡é•¿ï¼Œè¿›è¡Œåˆ‡åˆ†
            if len(section_text) > self.config.max_chunk_size:
                print(
                    f"  âœ‚ï¸  ç« èŠ‚ [{section_info['section']}] è¿‡é•¿ ({len(section_text)} å­—ç¬¦)ï¼Œè¿›è¡Œåˆ‡åˆ†..."
                )
                sub_chunks = self._split_text_with_overlap(
                    text=section_text,
                    source_file=source_file,
                    section=section_info["section"],
                    images=section_images,
                    page_number=section_info["page"],
                )
                chunks.extend(sub_chunks)
            else:
                # ç›´æ¥åˆ›å»º chunk
                if section_text.strip():
                    chunk = self._create_chunk(
                        text=section_text,
                        source_file=source_file,
                        section=section_info["section"],
                        images=section_images,
                        page_number=section_info["page"],
                    )
                    chunks.append(chunk)

        print(f"  âœ… ç”Ÿæˆ {len(chunks)} ä¸ªä¼˜åŒ–çš„æ–‡æ¡£å—")
        return chunks

    def _extract_docx_images(self, file_path: Path) -> List[str]:
        """ä» Word æ–‡æ¡£ä¸­æå–åµŒå…¥å›¾ç‰‡ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        extracted = []
        doc_name = file_path.stem

        try:
            with zipfile.ZipFile(file_path, "r") as z:
                for file_info in z.namelist():
                    if file_info.startswith("word/media/"):
                        # æå–å›¾ç‰‡
                        image_data = z.read(file_info)
                        image_name = f"{doc_name}_{Path(file_info).name}"
                        output_path = self.image_output_dir / image_name

                        with open(output_path, "wb") as f:
                            f.write(image_data)

                        extracted.append(str(output_path))
        except Exception as e:
            print(f"âš ï¸ æå–å›¾ç‰‡å¤±è´¥: {e}")

        return extracted

    # ==================== Markdown è§£æ ====================

    def _parse_markdown(self, file_path: Path) -> List[DocumentChunk]:
        """è§£æ Markdown æ–‡æ¡£"""
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()

        # æå–æ‰€æœ‰å›¾ç‰‡å¼•ç”¨
        image_pattern = r"!\[([^\]]*)\]\(([^)]+)\)"
        images_in_doc = re.findall(image_pattern, content)

        # æ„å»ºå›¾ç‰‡è·¯å¾„æ˜ å°„
        doc_dir = file_path.parent
        image_map = {}

        for alt_text, img_path in images_in_doc:
            full_path = doc_dir / img_path
            if full_path.exists():
                image_map[img_path] = str(full_path)

        # ä½¿ç”¨ä¼˜åŒ–çš„åˆ‡åˆ†ç­–ç•¥
        chunks = self._split_markdown_optimized(content, str(file_path), image_map)

        return chunks

    def _split_markdown_optimized(
        self, content: str, source_file: str, image_map: Dict[str, str]
    ) -> List[DocumentChunk]:
        """ä¼˜åŒ–çš„ Markdown åˆ‡åˆ†"""
        # ç¬¬ä¸€æ­¥ï¼šä¿æŠ¤ä»£ç å—ï¼Œé¿å…è¢«æ ‡é¢˜åˆ†å‰²æ‰“æ–­
        code_block_pattern = r"(```[\s\S]*?```)"
        code_blocks = []

        def protect_code_block(match):
            """å°†ä»£ç å—æ›¿æ¢ä¸ºå ä½ç¬¦"""
            code_blocks.append(match.group(0))
            return f"__CODE_BLOCK_{len(code_blocks) - 1}__"

        # ä¿æŠ¤ä»£ç å—
        protected_content = re.sub(code_block_pattern, protect_code_block, content)

        # æŒ‰æ ‡é¢˜åˆ†å‰²ï¼ˆæ”¯æŒ 1-3 çº§æ ‡é¢˜ï¼‰
        section_pattern = r"(^#{1,3}\s+.+$)"
        parts = re.split(section_pattern, protected_content, flags=re.MULTILINE)

        chunks = []
        current_section = "æ–‡æ¡£å¼€å§‹"
        current_text = []
        current_images = []

        def restore_code_blocks(text: str) -> str:
            """æ¢å¤ä»£ç å—"""
            result = text
            for i, code_block in enumerate(code_blocks):
                result = result.replace(f"__CODE_BLOCK_{i}__", code_block)
            return result

        for part in parts:
            part = part.strip()
            if not part:
                continue

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜
            if re.match(r"^#{1,3}\s+", part):
                # ä¿å­˜ä¹‹å‰çš„å—ï¼ˆä½¿ç”¨ä¼˜åŒ–åˆ‡åˆ†ï¼‰
                if current_text:
                    text = restore_code_blocks("\n".join(current_text))
                    sub_chunks = self._split_text_with_overlap(
                        text=text,
                        source_file=source_file,
                        section=current_section,
                        images=current_images.copy(),
                    )
                    chunks.extend(sub_chunks)
                    current_text = []
                    current_images = []

                current_section = re.sub(r"^#+\s*", "", part)
            else:
                # å…ˆæ¢å¤ä»£ç å—å†æå–å›¾ç‰‡
                restored_part = restore_code_blocks(part)

                # æå–è¯¥éƒ¨åˆ†ä¸­çš„å›¾ç‰‡
                img_refs = re.findall(r"!\[[^\]]*\]\(([^)]+)\)", restored_part)
                for img_ref in img_refs:
                    if (
                        img_ref in image_map
                        and image_map[img_ref] not in current_images
                    ):
                        current_images.append(image_map[img_ref])

                # æ¸…ç†å›¾ç‰‡å¼•ç”¨ä¸ºå ä½ç¬¦ï¼Œä½†ä¿ç•™ä»£ç å—
                clean_text = re.sub(
                    r"!\[([^\]]*)\]\([^)]+\)", r"[å›¾ç‰‡: \1]", restored_part
                )
                if clean_text.strip():
                    current_text.append(clean_text)

        # ä¿å­˜æœ€åä¸€ä¸ªå—
        if current_text:
            text = restore_code_blocks("\n".join(current_text))
            sub_chunks = self._split_text_with_overlap(
                text=text,
                source_file=source_file,
                section=current_section,
                images=current_images.copy(),
            )
            chunks.extend(sub_chunks)

        return chunks

    # ==================== PDF è§£æ ====================

    def _parse_pdf(self, file_path: Path) -> List[DocumentChunk]:
        """è§£æ PDF æ–‡æ¡£"""
        try:
            from unstructured.partition.pdf import partition_pdf
        except ImportError:
            raise ImportError('è¯·å®‰è£… PDF æ”¯æŒ: pip install "unstructured[pdf]"')

        elements = partition_pdf(
            filename=str(file_path),
            strategy="hi_res",
            extract_images_in_pdf=True,
            extract_image_block_types=["Image", "Table"],
            extract_image_block_output_dir=str(self.image_output_dir),
        )

        extracted_images = list(
            self.image_output_dir.glob(f"{file_path.stem}*.png")
        ) + list(self.image_output_dir.glob(f"{file_path.stem}*.jpg"))

        return self._elements_to_chunks_optimized(
            elements=elements,
            source_file=str(file_path),
            extracted_images=[str(p) for p in extracted_images],
        )

    # ==================== ä¼˜åŒ–çš„åˆ‡ç‰‡æ ¸å¿ƒç®—æ³• ====================

    def _elements_to_chunks_optimized(
        self,
        elements,
        source_file: str,
        extracted_images: List[str],
    ) -> List[DocumentChunk]:
        """
        ä¼˜åŒ–çš„å…ƒç´ è½¬æ¢æ–¹æ³• - æ”¯æŒå›¾ç‰‡åŸä½ç½®æ’å…¥

        ç­–ç•¥ï¼š
        1. æŒ‰ Title/Header åˆæ­¥åˆ†ç»„
        2. è¯†åˆ«è‡ªå®šä¹‰æ ‡é¢˜æ¨¡å¼ï¼ˆä¸­æ–‡æ ‡é¢˜ï¼‰
        3. è¯†åˆ« Image å…ƒç´ ï¼Œåœ¨åŸä½ç½®æ’å…¥ [IMG:path] å ä½ç¬¦
        4. å¯¹æ¯ç»„åº”ç”¨é•¿åº¦é™åˆ¶å’Œé‡å 
        """
        chunks = []
        current_section = "æ–‡æ¡£å¼€å§‹"
        current_page = 0
        image_index = 0  # å›¾ç‰‡ç´¢å¼•ï¼Œç”¨äºåŒ¹é…æå–çš„å›¾ç‰‡

        # ç¬¬ä¸€éï¼šæŒ‰æ ‡é¢˜åˆæ­¥åˆ†ç»„ï¼ŒåŒæ—¶å¤„ç†å›¾ç‰‡å ä½ç¬¦
        sections = []
        section_data = {
            "section": current_section,
            "texts": [],
            "page": 0,
            "images": [],  # è¯¥ section åŒ…å«çš„å›¾ç‰‡è·¯å¾„
        }

        for element in elements:
            element_type = element.category
            text = element.text.strip() if element.text else ""

            # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡å…ƒç´ 
            if element_type == "Image":
                # åœ¨æ–‡æœ¬ä¸­æ’å…¥å›¾ç‰‡å ä½ç¬¦
                if image_index < len(extracted_images):
                    img_path = extracted_images[image_index]
                    placeholder = f"[IMG:{img_path}]"
                    section_data["texts"].append(placeholder)
                    section_data["images"].append(img_path)
                    image_index += 1
                    print(
                        f"  ğŸ–¼ï¸  å‘ç°å›¾ç‰‡å…ƒç´ ï¼Œæ’å…¥å ä½ç¬¦: {os.path.basename(img_path)}"
                    )
                continue

            # æ ‡é¢˜è¯†åˆ«
            is_title = element_type in ["Title", "Header"]

            # é¢å¤–æ£€æŸ¥ï¼šä¸­æ–‡æ ‡é¢˜æ¨¡å¼
            if (
                not is_title
                and text
                and self.config.split_by_title
                and self._is_custom_title(text)
            ):
                is_title = True

            if is_title and text:
                # ä¿å­˜ä¹‹å‰çš„section
                if section_data["texts"]:
                    sections.append(section_data)

                # æ–°section
                current_section = text
                current_page = self._get_page_number(element)
                section_data = {
                    "section": current_section,
                    "texts": [],
                    "page": current_page,
                    "images": [],
                }

            elif text:
                # æ·»åŠ æ–‡æœ¬åˆ°å½“å‰section
                section_data["texts"].append(text)

        # ä¿å­˜æœ€åä¸€ä¸ªsection
        if section_data["texts"]:
            sections.append(section_data)

        # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°ä»»ä½•sectionï¼Œåˆ›å»ºé»˜è®¤section
        if not sections:
            all_texts = [
                e.text.strip() for e in elements if e.text and e.category != "Image"
            ]
            sections = [
                {
                    "section": "æ–‡æ¡£å†…å®¹",
                    "texts": all_texts,
                    "page": 0,
                    "images": extracted_images,
                }
            ]

        # å¤„ç†æœªè¢«åˆ†é…çš„å›¾ç‰‡ï¼ˆå¦‚æœ unstructured æ²¡æœ‰è¯†åˆ«å‡º Image å…ƒç´ ï¼‰
        remaining_images = (
            extracted_images[image_index:]
            if image_index < len(extracted_images)
            else []
        )
        if remaining_images:
            print(
                f"  âš ï¸  æœ‰ {len(remaining_images)} å¼ å›¾ç‰‡æœªèƒ½å®šä½åˆ°åŸå§‹ä½ç½®ï¼Œå°†å‡åŒ€åˆ†é…"
            )

        print(f"  ğŸ“Š è¯†åˆ«åˆ° {len(sections)} ä¸ªç« èŠ‚ï¼Œ{image_index} å¼ å›¾ç‰‡å·²å®šä½")

        # ç¬¬äºŒéï¼šå¯¹æ¯ä¸ªsectionåº”ç”¨é•¿åº¦é™åˆ¶å’Œé‡å 
        for section_info in sections:
            section_text = "\n".join(section_info["texts"])
            section_images = section_info.get("images", [])

            # å¦‚æœsectionè¿‡é•¿ï¼Œè¿›è¡Œåˆ‡åˆ†
            if len(section_text) > self.config.max_chunk_size:
                print(
                    f"  âœ‚ï¸  ç« èŠ‚ [{section_info['section']}] è¿‡é•¿ ({len(section_text)} å­—ç¬¦)ï¼Œè¿›è¡Œåˆ‡åˆ†..."
                )
                sub_chunks = self._split_text_with_overlap(
                    text=section_text,
                    source_file=source_file,
                    section=section_info["section"],
                    images=section_images,  # ä¼ é€’è¯¥ section çš„å›¾ç‰‡
                    page_number=section_info["page"],
                )
                chunks.extend(sub_chunks)
            else:
                # ç›´æ¥åˆ›å»ºchunk
                if section_text.strip():
                    chunk = self._create_chunk(
                        text=section_text,
                        source_file=source_file,
                        section=section_info["section"],
                        images=section_images,
                        page_number=section_info["page"],
                    )
                    chunks.append(chunk)

        # ç¬¬ä¸‰æ­¥ï¼šå¤„ç†å‰©ä½™æœªå®šä½çš„å›¾ç‰‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if remaining_images and chunks:
            chunks = self._distribute_images_to_chunks(chunks, remaining_images)

        print(f"  âœ… ç”Ÿæˆ {len(chunks)} ä¸ªä¼˜åŒ–çš„æ–‡æ¡£å—")
        return chunks

    def _is_custom_title(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ¹é…è‡ªå®šä¹‰æ ‡é¢˜æ¨¡å¼"""
        for pattern in self.config.title_patterns:
            if re.match(pattern, text.strip()):
                return True
        return False

    def _split_text_with_overlap(
        self,
        text: str,
        source_file: str,
        section: str,
        images: List[str] = None,
        page_number: int = 0,
    ) -> List[DocumentChunk]:
        """
        ä½¿ç”¨é‡å ç­–ç•¥åˆ‡åˆ†é•¿æ–‡æœ¬

        Args:
            text: è¦åˆ‡åˆ†çš„æ–‡æœ¬
            source_file: æºæ–‡ä»¶
            section: ç« èŠ‚å
            images: å…³è”å›¾ç‰‡
            page_number: é¡µç 

        Returns:
            åˆ‡åˆ†åçš„ chunk åˆ—è¡¨
        """
        if images is None:
            images = []

        chunks = []

        # ç¬¬ä¸€æ­¥ï¼šä¿æŠ¤ä»£ç å—ï¼Œé¿å…è¢«åˆ†å‰²æ‰“æ–­
        code_block_pattern = r"(```[\s\S]*?```)"
        code_blocks = []

        def protect_code_block(match):
            """å°†ä»£ç å—æ›¿æ¢ä¸ºå ä½ç¬¦"""
            code_blocks.append(match.group(0))
            return f"__CODE_BLOCK_{len(code_blocks) - 1}__"

        protected_text = re.sub(code_block_pattern, protect_code_block, text)

        def restore_code_blocks(t: str) -> str:
            """æ¢å¤ä»£ç å—"""
            result = t
            for i, code_block in enumerate(code_blocks):
                result = result.replace(f"__CODE_BLOCK_{i}__", code_block)
            return result

        # å¦‚æœæ–‡æœ¬ä¸è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œç›´æ¥è¿”å›
        if len(text) <= self.config.max_chunk_size:
            chunk = self._create_chunk(
                text=text,
                source_file=source_file,
                section=section,
                images=images,
                page_number=page_number,
            )
            chunks.append(chunk)
            return chunks

        # ç­–ç•¥1: å°è¯•æŒ‰æ®µè½åˆ‡åˆ†ï¼ˆä½¿ç”¨ä¿æŠ¤åçš„æ–‡æœ¬ï¼‰
        if self.config.split_by_paragraph:
            paragraphs = protected_text.split("\n")
            paragraphs = [p.strip() for p in paragraphs if p.strip()]

            current_chunk_text = []
            current_length = 0

            for para in paragraphs:
                # æ£€æŸ¥æ˜¯å¦æ˜¯ä»£ç å—å ä½ç¬¦ï¼ˆä¸åº”è¯¥è¢«åˆ†å‰²ï¼‰
                is_code_placeholder = para.startswith(
                    "__CODE_BLOCK_"
                ) and para.endswith("__")
                para_len = len(para)

                # å¦‚æœæ˜¯ä»£ç å—å ä½ç¬¦ï¼Œè·å–å®é™…é•¿åº¦
                actual_para_len = para_len
                if is_code_placeholder:
                    try:
                        idx = int(para.replace("__CODE_BLOCK_", "").replace("__", ""))
                        actual_para_len = (
                            len(code_blocks[idx])
                            if idx < len(code_blocks)
                            else para_len
                        )
                    except ValueError:
                        pass

                # å¦‚æœå•ä¸ªæ®µè½å°±è¶…é•¿ï¼Œéœ€è¦å¼ºåˆ¶åˆ‡åˆ†ï¼ˆä½†ä»£ç å—ä½œä¸ºæ•´ä½“ä¿ç•™ï¼‰
                if (
                    actual_para_len > self.config.max_chunk_size
                    and not is_code_placeholder
                ):
                    # ä¿å­˜å½“å‰ç´¯ç§¯çš„
                    if current_chunk_text:
                        chunk_text = restore_code_blocks("\n".join(current_chunk_text))
                        chunk = self._create_chunk(
                            text=chunk_text,
                            source_file=source_file,
                            section=section,
                            images=images if not chunks else [],
                            page_number=page_number,
                        )
                        chunks.append(chunk)
                        current_chunk_text = []
                        current_length = 0

                    # å¼ºåˆ¶åˆ‡åˆ†è¶…é•¿æ®µè½
                    restored_para = restore_code_blocks(para)
                    sub_chunks = self._split_by_characters(
                        restored_para, source_file, section, [], page_number
                    )
                    chunks.extend(sub_chunks)

                # å¦‚æœåŠ ä¸Šè¿™ä¸ªæ®µè½ä¼šè¶…é•¿
                elif current_length + actual_para_len > self.config.max_chunk_size:
                    # ä¿å­˜å½“å‰chunk
                    if current_chunk_text:
                        chunk_text = restore_code_blocks("\n".join(current_chunk_text))
                        chunk = self._create_chunk(
                            text=chunk_text,
                            source_file=source_file,
                            section=section,
                            images=images if not chunks else [],
                            page_number=page_number,
                        )
                        chunks.append(chunk)

                    # æ·»åŠ é‡å éƒ¨åˆ†ï¼ˆä»£ç å—ä¸å‚ä¸é‡å ï¼‰
                    if (
                        self.config.chunk_overlap > 0
                        and current_chunk_text
                        and not is_code_placeholder
                    ):
                        overlap_text = current_chunk_text[-1]
                        current_chunk_text = [overlap_text, para]
                        current_length = len(overlap_text) + actual_para_len
                    else:
                        current_chunk_text = [para]
                        current_length = actual_para_len
                else:
                    current_chunk_text.append(para)
                    current_length += actual_para_len

            # ä¿å­˜æœ€åä¸€ä¸ªchunk
            if current_chunk_text:
                chunk_text = restore_code_blocks("\n".join(current_chunk_text))
                chunk = self._create_chunk(
                    text=chunk_text,
                    source_file=source_file,
                    section=section,
                    images=images if not chunks else [],
                    page_number=page_number,
                )
                chunks.append(chunk)

        else:
            # ç­–ç•¥2: æŒ‰å›ºå®šå­—ç¬¦æ•°åˆ‡åˆ†
            chunks = self._split_by_characters(
                text, source_file, section, images, page_number
            )

        return chunks

    def _split_by_characters(
        self,
        text: str,
        source_file: str,
        section: str,
        images: List[str],
        page_number: int,
    ) -> List[DocumentChunk]:
        """æŒ‰å›ºå®šå­—ç¬¦æ•°åˆ‡åˆ†ï¼ˆå¸¦é‡å ï¼‰"""
        chunks = []
        start = 0
        text_length = len(text)

        while start < text_length:
            end = min(start + self.config.max_chunk_size, text_length)

            # å°½é‡åœ¨å¥å·ã€é—®å·ã€æ„Ÿå¹å·å¤„åˆ‡åˆ†
            if end < text_length:
                for sep in ["ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?"]:
                    last_sep = text[start:end].rfind(sep)
                    if last_sep > self.config.max_chunk_size * 0.7:  # è‡³å°‘è¾¾åˆ°70%é•¿åº¦
                        end = start + last_sep + 1
                        break

            chunk_text = text[start:end].strip()

            if chunk_text:
                chunk = self._create_chunk(
                    text=chunk_text,
                    source_file=source_file,
                    section=section,
                    images=images if not chunks else [],
                    page_number=page_number,
                )
                chunks.append(chunk)

            # è®¡ç®—ä¸‹ä¸€ä¸ªèµ·ç‚¹ï¼ˆè€ƒè™‘é‡å ï¼‰
            if self.config.chunk_overlap > 0 and end < text_length:
                start = max(start + 1, end - self.config.chunk_overlap)
            else:
                start = end

        return chunks

    def _distribute_images_to_chunks(
        self, chunks: List[DocumentChunk], images: List[str]
    ) -> List[DocumentChunk]:
        """
        æ™ºèƒ½åˆ†é…å›¾ç‰‡åˆ°å„ä¸ªchunk

        ç­–ç•¥ï¼šæ ¹æ®chunkæ•°é‡å‡åŒ€åˆ†é…ï¼Œæˆ–è€…é›†ä¸­åˆ†é…åˆ°ç¬¬ä¸€ä¸ªchunk
        """
        if not images or not chunks:
            return chunks

        if self.config.distribute_images_evenly:
            # å‡åŒ€åˆ†é…
            images_per_chunk = max(1, len(images) // len(chunks))
            for i, chunk in enumerate(chunks):
                start_idx = i * images_per_chunk
                end_idx = start_idx + images_per_chunk
                if i == len(chunks) - 1:  # æœ€åä¸€ä¸ªchunkè·å–å‰©ä½™æ‰€æœ‰å›¾ç‰‡
                    end_idx = len(images)
                chunk.images = images[start_idx:end_idx]
                chunk.metadata["image_count"] = len(chunk.images)
                chunk.metadata["has_images"] = len(chunk.images) > 0
        else:
            # é›†ä¸­åˆ†é…åˆ°ç¬¬ä¸€ä¸ªchunk
            chunks[0].images = images
            chunks[0].metadata["image_count"] = len(images)
            chunks[0].metadata["has_images"] = True

        return chunks

    def _create_chunk(
        self,
        text: str,
        source_file: str,
        section: str,
        images: List[str],
        page_number: int,
    ) -> DocumentChunk:
        """åˆ›å»ºæ–‡æ¡£å—"""
        chunk_id = hashlib.md5(
            f"{source_file}:{section}:{text[:100]}".encode()
        ).hexdigest()[:12]

        return DocumentChunk(
            chunk_id=chunk_id,
            text=text,
            source_file=source_file,
            section=section,
            page_number=page_number,
            images=images,
            metadata={
                "has_images": len(images) > 0,
                "image_count": len(images),
                "text_length": len(text),
            },
        )

    def _get_page_number(self, element) -> int:
        """è·å–å…ƒç´ æ‰€åœ¨é¡µç """
        if hasattr(element, "metadata") and hasattr(element.metadata, "page_number"):
            return element.metadata.page_number or 0
        return 0
